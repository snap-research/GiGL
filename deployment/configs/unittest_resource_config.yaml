# GiglResourceConfig for our unit tests.
shared_resource_config:
  # Resource labels are just compute labels that should be attached to all compute resources spun up by GiGL.
  # So a practitioner can have a more fine grained understanding of resource utilization and cost of the resources in their GCP billing.
  # Read more here: https://cloud.google.com/compute/docs/labeling-resources#what-are-labels
  resource_labels:
  # We have a 63 character limit for cost_resource_group_tag.

  # COMPONENT is one of {pre|sgs|spl|tra|inf|pos} standing for:
  #   {Preprocessor | Subgraph Sampler | Split Generator | Trainer | Inference
  #   | Post  Processor} so we can get more accurate cost measurements
  #   of each component. This will be automatically filled in code.
    cost_resource_group_tag: dev_experiments_COMPONENT
    cost_resource_group: gigl_platform
  common_compute_config:
    project: "external-snap-ci-github-gigl"
    region: "us-central1"
    # For test, we use temp assets bucket and bq dataset since these assets should be short lived
    # by default. Since, we are not using them for development.
    temp_assets_bucket: "gs://gigl-cicd-temp"
    temp_regional_assets_bucket: "gs://gigl-cicd-temp"
    perm_assets_bucket: "gs://gigl-cicd-temp" # For testing, we don't persist models, configs, etc. - they should expire w/ TTL
    temp_assets_bq_dataset_name: "gigl_temp_assets"
    embedding_bq_dataset_name: "gigl_temp_assets"
    gcp_service_account_email: "untrusted-external-github-gigl@external-snap-ci-github-gigl.iam.gserviceaccount.com"
    dataflow_runner: "DirectRunner"
preprocessor_config:
  edge_preprocessor_config:
    num_workers: 1
    max_num_workers: 128
    machine_type: "n2d-highmem-32"
    disk_size_gb: 300
  node_preprocessor_config:
    num_workers: 1
    max_num_workers: 128
    machine_type: "n2d-highmem-64"
    disk_size_gb: 300
subgraph_sampler_config:
  machine_type: "n2d-highmem-16"
  num_local_ssds: 2
  num_replicas: 4
split_generator_config:
  machine_type: n2d-standard-16
  num_local_ssds: 2
  num_replicas: 4
trainer_config:
  vertex_ai_trainer_config:
    machine_type: "n1-highmem-8"
    gpu_type: nvidia-tesla-k80
    gpu_limit: 1
    num_replicas: 2
inferencer_config:
  num_workers: 1
  max_num_workers: 256
  machine_type: "c3-standard-22"
  disk_size_gb: 100
