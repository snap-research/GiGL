## Trainer

The Trainer component reads the outputs of split generator (which paths are specified in the frozen config), and trains a GNN model on the training set, early stops on the performance of the validation set, and finally evaluates on the test set. The training logic is implemented with PyTorch Distributed Data Parallel (DDP) Training, which enables distributed training on multiple GPU cards across multiple worker nodes.

## Input

- **job_name** (AppliedTaskIdentifier):  which uniquely identifies an end-to-end task.
- **task_config_uri** (Uri):  Path which points to a "frozen" `GbmlConfig` proto yaml file - Can be either manually created, or `config_populator` component (recommended approach) can be used which can generate this frozen config from a template config.
- **resource_config_uri** (Uri): Path which points to a `GiGLResourceConfig` yaml

## What does it do?

The whole model training contains two main components: (i) the Trainer, which that sets up the environment, and (ii) a user-defined instance of `BaseTrainer` that contains the actual training loop w.r.t. the given task. For example, for node anchor-based link prediction, we have `NodeAnchorBasedLinkPredictionModelingTaskSpec`. Model training involves the following steps:

- The Trainer sets up the (optionally distributed) Torch training environment.
- The Trainer reads `GraphMetadata` that was generated by the Data Preprocessor.
- The Trainer initializes the `BaseTrainer` instance (instance specified at the `trainerClsPath` field in the `trainerConfig` section of the frozen `GbmlConfig`, and with arguments at `trainerArgs`) and initializes the GNN model.

- We start model training as indicated by the `BaseTrainer` instance.  This may look something like:
    - We initialize training and validation dataloaders (See: `NodeAnchorBasedLinkPredictionDatasetDataloaders` in [dataset_metadata_utils.py](../../python/gigl/src/common/types/pb_wrappers/dataset_metadata_utils.py))
    - Follow a standard distributed training scheme: each worker loads a batch of data and performs the normal forward and backward passes for model training in a distributed way. 
    - Every fixed number of training batches(`val_every_num_batches`), we evaluate the current model on the validation set with a fixed number of validation batches (`num_val_batches`)
    - We follow a standard early-stopping strategy on the validation performances on offline metrics, with a configurable patience parameter (`early_stop_patience`) or see `EarlyStopper` utility class in [early_stop.py](../../python/gigl/src/common/modeling_task_specs/utils/early_stop.py)
    - When early-stopping is triggered to end the training process, we reload the saved model at the best validation batch, and run evaluation (test) it with a fixed number of test batches (`num_test_batches`).  
    - At the end, we return the model and its test performance (offline metrics) back to the Trainer.

- The Trainer persists output metadata like model parameters and offline metrics (see [Output](#output)).

## How do I run it?

**Import GiGL**

```python
from gigl.src.split_generator.split_generator import SplitGenerator
from gigl.common import UriFactory
from gigl.src.common.types import AppliedTaskIdentifier

trainer = Trainer()

trainer.run(
    applied_task_identifier=AppliedTaskIdentifier("sample_job_name"),
    task_config_uri=UriFactory.create_uri("gs://MY TEMP ASSETS BUCKET/frozen_task_config.yaml"),
    resource_config_uri=UriFactory.create_uri("gs://MY TEMP ASSETS BUCKET/resource_config.yaml")
)
```

Note: If you are training on VertexAI and using a custom class, you will have to provide a docker image (Either `cuda_docker_uri` for GPU training or `cpu_docker_uri` for CPU training.)


**Command Line**

```
python -m \
    gigl.src.training.trainer \
    --job_name="sample_job_name" \
    --task_config_uri="gs://MY TEMP ASSETS BUCKET/frozen_task_config.yaml"
    --resource_config_uri="gs://MY TEMP ASSETS BUCKET/resource_config.yaml"
```

## Output

Ater the training process finishes:

- The Trainer saves the trained modelâ€™s `state_dict` at specified location (`trainedModelUri` field of `sharedConfig.trainedModelMetadata`).

- The trainer logs training metrics to `trainingLogsUri` field of `sharedConfig.trainedModelMetadata`. To view the metrics on your local, you can run the command: `tensorboard --logdir gs://tensorboard_logs_uri_here`

## Custom Usage

The Trainer is designed to be task-agnostic, with the detailed model and training logics specified in the user-provided `BaseTrainer` instance.  Modifying the `BaseTrainer` instance allows maximal flexibility in changing model architecture and training parameters.  

## Other

### Torch Profiler

You can profile trainer performance metrics, such as gpu/cpu utilization by adding below to task_config.yaml

```
profilerConfig:
    should_enable_profiler: true
    profiler_log_dir: gs://path_to_my_bucket  (or a local dir)
    profiler_args:
        wait:'0'
        with_stack: 'True'
```


### Monitoring and logging

Once the trainer component starts, the training process can be monitored via the gcloud console under Vertex AI Custom Jobs (`https://console.cloud.google.com/vertex-ai/training/custom-jobs?project=<project_name_here>`). You can also view the job name, status, jobspec, and more using `gcloud ai custom-jobs list --project <project_name_here>`

On the Vertex AI UI, you can see all the information like machine/acceleratior information, CPU Utilization, GPU utiliization, Network data etc. Here, you will also find the "View logs" tab, which will open the Stackdriver for your job which logs everything from your modeling task spec as the training progresses in real time. 

If you would like to view the logs locally, you can also use: `gcloud ai custom-jobs stream-logs <custom job ID> --project=<project_name_here> --region=<region here>`.

### Parameters

We provide some base class implimentations for training.
See:
- `python/gigl/src/common/modeling_task_specs/graphsage_template_modeling_spec.py`
- `python/gigl/src/common/modeling_task_specs/node_anchor_based_link_prediction_modeling_task_spec.py`
- `python/gigl/src/common/modeling_task_specs/node_classification_modeling_task_spec.py`

**** Note: many training/model params require dep on using the right model / training setup i.e. specific configurations may not be supported - see individual implementations to understand how each param is used. Training specs are fully customizable - these are only examples

They all provide runtime arguments similar to below that can help with your model training behaviour/configs. We present the args and short example below for params that might be passed in for ABLP type training:

- Training environment parameters (number of workers for different dataloaders)
    - train_main_num_workers
    - train_random_negative_num_workers
    - val_main_num_workers
    - val_random_negative_num_workers
    - test_main_num_workers
    - test_random_negative_num_workers
  
  Note that training involves multiple dataloaders simultaneously.  Take care to specify these parameters in a way which avoids overburdening your machine.  It is recommended to specify `(train_main_sample_num_workers + train_random_sample_num_workers + val_main_sample_num_workers + val_random_sample_num_workers < num_cpus)`, and `(test_main_sample_num_workers + test_random_sample_num_workers < num_cpus)` to avoid training stalling due to contention.

- Modifying the GNN model:
    - Specified by arg `gnn_model_class_path`
        - Some Sample GNN models are defined [here](/python/gigl/src/common/models/pyg/homogeneous.py) and initialized in the `init_model` function in ModelingTaskSpec. When trying different GNN models, it is recommended to also include the new GNN architectures under the same file and declare them as is currently done. This cannot currently be done from the default `GbmlConfig` yaml.  



- Non Exhaustive list of Model parameters:
    - hidden_dim: dimension of the hidden layers
    - num_layers: number of layers in the GNN (this should be the same as numHops under subgraphSamplerConfig)
    - out_channels: dimension of the output embeddings
    - should_l2_normalize_embedding_layer_output: whether apply L2 normalization on the output embeddings

- Non Exhaustive list of Training parameters:
    - num_heads
    - val_every_num_batches: validation frequence per training batches
    - num_val_batches: number of validation batches
    - num_test_batches: number of testing batches
    - optim_class_path: defaults to "torch.optim.Adam"
    - optim_lr: learning rate of the optimizer
    - optim_weight_decay: weight decay of the optimizer
    - clip_grad_norm
    - lr_scheduler_name: defaults to "torch.optim.lr_scheduler.ConstantLR"
    - factor: param for lr scheduler
    - total_iters: param for lr scheduler
    - main_sample_batch_size: training batch size
    - random_negative_sample_batch_size: random negative sample batch size for training
    - random_negative_sample_batch_size_for_evaluation: random negative sample batch size for evaluation
    - train_main_num_workers
    - val_main_num_workers
    - test_main_num_workers
    - train_random_negative_num_workers
    - val_random_negative_num_workers
    - test_random_negative_num_workers
    - early_stop_criterion: defaults to "loss"
    - early_stop_patience: patience for earlystopping
    - task_path: python class path to supported training tasks i.e. Retrieval `gigl.src.common.models.layers.task.Retrieval`; see gigl.src.common.models.layers.task.py for more info
    - softmax_temp: temperature parameter in the `softmax` loss
    - should_remove_accidental_hits



###  Background for distributed training

Trainer currently uses PyTorch distributed training abstractions to enable multi-node and multi-GPU training. Some useful terminology and links to learn about these abstractions below.

- **WORLD**: Group of processes/workers that are used for distributed training.
- **WORLD_SIZE**:  The number of processes/workers in the distributed training WORLD.
- **RANK**: The unique id (usually index) of the process/worker in the distributed training WORLD.
- **Data loader worker**: A worker used specifically for loading data; if the dataloader worker is utilizing the same thread/process as a worker in distributed training WORLD, then we may incur blocking execution of training, resulting in slowdowns.

- **[Distributed Data Parallel](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)**: Pytorch's version of [Data parallalism](https://en.wikipedia.org/wiki/Data_parallelism) across different **processes** (could even be processes on different machines), to speed up traiing on large datasets. 


- **[TORCH.DISTRIBUTED package](https://pytorch.org/docs/stable/distributed.html)**: A torch package containing tools for distributed communication and trainings.
    - Defines [backends for distributed communication](https://pytorch.org/docs/stable/distributed.html#backends) like `gloo` and `nccl` - as a ML practitioner you should not worry about how these work, but important to know what **devices** and **collective functions** they support.
    - Contains **"[Collective functions](https://pytorch.org/docs/stable/distributed.html#collective-functions)"** like `torch.distributed.broadcast`, `torch.distributed.all_gather`, et al. which allow communication of tensors across the **WORLD**. 

